{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# vgg model , neural network to be trained on\n",
        "import torch.nn as nn\n",
        "\n",
        "_cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def _make_layers(cfg):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for layer_cfg in cfg:\n",
        "        if layer_cfg == 'M':\n",
        "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        else:\n",
        "            layers.append(nn.Conv2d(in_channels=in_channels,\n",
        "                                    out_channels=layer_cfg,\n",
        "                                    kernel_size=3,\n",
        "                                    stride=1,\n",
        "                                    padding=1,\n",
        "                                    bias=True))\n",
        "            layers.append(nn.BatchNorm2d(num_features=layer_cfg))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            in_channels = layer_cfg\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class _VGG(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG module for 3x32x32 input, 10 classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name='VGG11'):\n",
        "        super(_VGG, self).__init__()\n",
        "        cfg = _cfg[name]\n",
        "        self.layers = _make_layers(cfg)\n",
        "        flatten_features = 512\n",
        "        self.fc1 = nn.Linear(flatten_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.layers(x)\n",
        "        y = y.view(y.size(0), -1)\n",
        "        y = self.fc1(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "def VGG11():\n",
        "    return _VGG('VGG11')"
      ],
      "metadata": {
        "id": "f89ubm5asT69"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: to print the 20 mini batchesand update the training loop for one epoch\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import copy\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import logging\n",
        "import random\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "torch.set_num_threads(4)\n",
        "\n",
        "batch_size = 256 # batch for one node\n",
        "def train_model(model, train_loader, optimizer, criterion, epoch):\n",
        "    \"\"\"\n",
        "    model (torch.nn.module): The model created to train\n",
        "    train_loader (pytorch data loader): Training data loader\n",
        "    optimizer (optimizer.*): A instance of some sort of optimizer, usually SGD\n",
        "    criterion (nn.CrossEntropyLoss) : Loss function used to train the network\n",
        "    epoch (int): Current epoch number\n",
        "    \"\"\"\n",
        "    for epoch in range(1\n",
        "                       ):\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, data in enumerate(train_loader, 0):\n",
        "        #  data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # parameter gradients do it zero\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if batch_idx % 20 == 19:    # print every 20 mini-batches\n",
        "                print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {running_loss / 20:.3f}')\n",
        "                running_loss = 0.0\n",
        "    print('Finished Training')\n",
        "    return None\n",
        "\n",
        "def test_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = correct / len(test_loader.dataset) * 100\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def main():\n",
        "    learning_rate =0.01\n",
        "    weight_decay = 1e-4\n",
        "    #weight_decay = 0\n",
        "   # normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                #std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "    #normalize = transforms.Normalize(mean=[x/255.0 for x in [0.4914, 0.4822, 0.4465]],\n",
        "            #std=[x/255.0 for x in [0.2023, 0.1994, 0.2010]])\n",
        "\n",
        "    # from the references changes the normalize to achieve highest accuracy for epoch 1\n",
        "    normalize=transforms.Normalize(\n",
        "        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) )\n",
        "    transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "            ])\n",
        "    # transform_train=transforms.Compose([\n",
        "    # transforms.Resize(size=(224, 224)),\n",
        "    # transforms.ToTensor(),normalize,\n",
        "    #         ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize])\n",
        "    training_set = datasets.CIFAR10(root=\"./data\", train=True,\n",
        "                                                download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(training_set,\n",
        "                                                    num_workers=2,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    sampler=None,\n",
        "                                                    shuffle=True,\n",
        "                                                    pin_memory=True)\n",
        "    test_set = datasets.CIFAR10(root=\"./data\", train=False,\n",
        "                                download=True, transform=transform_test)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                              num_workers=2,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              pin_memory=True)\n",
        "    training_criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    model = VGG11() # calling the model\n",
        "    model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay,nesterov=True)\n",
        "    training_criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', factor=0.5) # tried anothe schedular\n",
        "\n",
        "    # running training for one epoch\n",
        "    for epoch in range(1):\n",
        "        train_model(model, train_loader, optimizer, training_criterion, epoch)\n",
        "        test_acc = test_model(model, test_loader, training_criterion)\n",
        "        scheduler.step(test_acc)  # Update the learning rate scheduler\n",
        "\n",
        "    if test_acc is not None:\n",
        "        print(f\"Highest accuracy achieved: {test_acc:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sslJzrLdyl9g",
        "outputId": "f51425df-956f-406b-c3fb-ccb5bdda433c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,    20] loss: 2.014\n",
            "[1,    40] loss: 1.640\n",
            "[1,    60] loss: 1.421\n",
            "[1,    80] loss: 1.343\n",
            "[1,   100] loss: 1.250\n",
            "[1,   120] loss: 1.207\n",
            "[1,   140] loss: 1.103\n",
            "[1,   160] loss: 1.056\n",
            "[1,   180] loss: 1.024\n",
            "Finished Training\n",
            "Test set: Average loss: 0.0056, Accuracy: 5348/10000 (53.48%)\n",
            "\n",
            "Highest accuracy achieved: 53.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import copy\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import logging\n",
        "import random\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "\n",
        "\n",
        "#import model as mdl\n",
        "device = \"cuda\"\n",
        "torch.set_num_threads(4)\n",
        "\n",
        "batch_size = 256 # batch for one node\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "\n",
        "    # initialize the process group\n",
        "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 20 == 19:\n",
        "            print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {running_loss / 20:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    return None\n",
        "\n",
        "def test_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main():\n",
        "    rank = 0\n",
        "    world_size = 1\n",
        "    setup(rank, world_size)\n",
        "\n",
        "    normalize=transforms.Normalize(\n",
        "        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) )\n",
        "    transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "            ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize])\n",
        "    training_set = datasets.CIFAR10(root=\"./data\", train=True,\n",
        "                                    download=True, transform=transform_train)\n",
        "\n",
        "    # Training sampler\n",
        "    train_sampler = DistributedSampler(training_set, num_replicas=world_size, rank=rank)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(training_set,\n",
        "                                               num_workers=2,\n",
        "                                               batch_size=batch_size,\n",
        "                                               sampler=train_sampler,\n",
        "                                               shuffle=False,\n",
        "                                               pin_memory=True)\n",
        "    test_set = datasets.CIFAR10(root=\"./data\", train=False,\n",
        "                                download=True, transform=transform_test)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                              num_workers=2,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              pin_memory=True)\n",
        "    training_criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    model = VGG11()\n",
        "    model.to(device)\n",
        "    # Adding DDP model\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01,\n",
        "                          momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    for epoch in range(1):\n",
        "        train_model(model, train_loader, optimizer, training_criterion, epoch)\n",
        "        test_model(model, test_loader, training_criterion)\n",
        "\n",
        "    cleanup()\n",
        "    scheduler.step()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3QbzYk9sR4w",
        "outputId": "fa6c87ce-d425-4913-c63c-cb5ce86ce65d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,    20] loss: 1.934\n",
            "[1,    40] loss: 1.627\n",
            "[1,    60] loss: 1.523\n",
            "[1,    80] loss: 1.462\n",
            "[1,   100] loss: 1.319\n",
            "[1,   120] loss: 1.242\n",
            "[1,   140] loss: 1.187\n",
            "[1,   160] loss: 1.155\n",
            "[1,   180] loss: 1.112\n",
            "Test set: Average loss: 1.0772, Accuracy: 6212/10000 (62%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}