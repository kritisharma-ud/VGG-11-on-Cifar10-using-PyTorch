{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tmLsgzzY6N3K"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "_cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def _make_layers(cfg):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for layer_cfg in cfg:\n",
        "        if layer_cfg == 'M':\n",
        "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        else:\n",
        "            layers.append(nn.Conv2d(in_channels=in_channels,\n",
        "                                    out_channels=layer_cfg,\n",
        "                                    kernel_size=3,\n",
        "                                    stride=1,\n",
        "                                    padding=1,\n",
        "                                    bias=True))\n",
        "            layers.append(nn.BatchNorm2d(num_features=layer_cfg))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            in_channels = layer_cfg\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class _VGG(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG module for 3x32x32 input, 10 classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name='VGG11'):\n",
        "        super(_VGG, self).__init__()\n",
        "        cfg = _cfg[name]\n",
        "        self.layers = _make_layers(cfg)\n",
        "        flatten_features = 512\n",
        "        self.fc1 = nn.Linear(flatten_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.layers(x)\n",
        "        y = y.view(y.size(0), -1)\n",
        "        y = self.fc1(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "def VGG11():\n",
        "    return _VGG('VGG11')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: 40 minibatches\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import copy\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import logging\n",
        "import random\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "torch.set_num_threads(4)\n",
        "\n",
        "batch_size = 256 # batch for one node\n",
        "def train_model(model, train_loader, optimizer, criterion, epoch):\n",
        "    \"\"\"\n",
        "    model (torch.nn.module): The model created to train\n",
        "    train_loader (pytorch data loader): Training data loader\n",
        "    optimizer (optimizer.*): A instance of some sort of optimizer, usually SGD\n",
        "    criterion (nn.CrossEntropyLoss) : Loss function used to train the network\n",
        "    epoch (int): Current epoch number\n",
        "    \"\"\"\n",
        "    for epoch in range(1):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if batch_idx % 40 == 39:    # print every 20 mini-batches\n",
        "                print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {running_loss / 20:.3f}')\n",
        "                running_loss = 0.0\n",
        "    print('Finished Training')\n",
        "    return None\n",
        "\n",
        "def test_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = correct / len(test_loader.dataset) * 100\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def main():\n",
        "    learning_rate =0.01\n",
        "    weight_decay = 1e-4\n",
        "    #weight_decay = 0\n",
        "   # normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                #std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "    #normalize = transforms.Normalize(mean=[x/255.0 for x in [0.4914, 0.4822, 0.4465]],\n",
        "            #std=[x/255.0 for x in [0.2023, 0.1994, 0.2010]])\n",
        "    normalize=transforms.Normalize(\n",
        "        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) )\n",
        "    transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "            ])\n",
        "    # transform_train=transforms.Compose([\n",
        "    # transforms.Resize(size=(224, 224)),\n",
        "    # transforms.ToTensor(),normalize,\n",
        "    #         ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize])\n",
        "    training_set = datasets.CIFAR10(root=\"./data\", train=True,\n",
        "                                                download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(training_set,\n",
        "                                                    num_workers=2,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    sampler=None,\n",
        "                                                    shuffle=True,\n",
        "                                                    pin_memory=True)\n",
        "    test_set = datasets.CIFAR10(root=\"./data\", train=False,\n",
        "                                download=True, transform=transform_test)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                              num_workers=2,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              pin_memory=True)\n",
        "    training_criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    model = VGG11()\n",
        "    model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay,nesterov=True)\n",
        "    training_criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', factor=0.5)\n",
        "\n",
        "    # running training for one epoch\n",
        "    for epoch in range(1):\n",
        "        train_model(model, train_loader, optimizer, training_criterion, epoch)\n",
        "        test_acc = test_model(model, test_loader, training_criterion)\n",
        "        scheduler.step(test_acc)  # Update the learning rate scheduler\n",
        "\n",
        "    if test_acc is not None:\n",
        "        print(f\"Highest accuracy achieved: {test_acc:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elXRWQKtG20Z",
        "outputId": "f6f213b3-09fa-4c75-f054-5a91434f4c94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,    40] loss: 3.613\n",
            "[1,    80] loss: 2.792\n",
            "[1,   120] loss: 2.414\n",
            "[1,   160] loss: 2.195\n",
            "Finished Training\n",
            "Test set: Average loss: 0.0047, Accuracy: 6099/10000 (60.99%)\n",
            "\n",
            "Highest accuracy achieved: 60.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanup()"
      ],
      "metadata": {
        "id": "kMiZmsVRJL1H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part2 40mini batches- 40 epoch\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import copy\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import logging\n",
        "import random\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "\n",
        "\n",
        "#import model as mdl\n",
        "device = \"cuda\"\n",
        "torch.set_num_threads(4)\n",
        "\n",
        "batch_size = 256 # batch for one node\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "\n",
        "    # initialize the process group\n",
        "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 40 == 39:\n",
        "            print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {running_loss / 40:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    return None\n",
        "\n",
        "def test_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main():\n",
        "    rank = 0\n",
        "    world_size = 1\n",
        "    setup(rank, world_size)\n",
        "\n",
        "    normalize=transforms.Normalize(\n",
        "        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) )\n",
        "    transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "            ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize])\n",
        "    training_set = datasets.CIFAR10(root=\"./data\", train=True,\n",
        "                                    download=True, transform=transform_train)\n",
        "\n",
        "    # Training sampler\n",
        "    train_sampler = DistributedSampler(training_set, num_replicas=world_size, rank=rank)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(training_set,\n",
        "                                               num_workers=2,\n",
        "                                               batch_size=batch_size,\n",
        "                                               sampler=train_sampler,\n",
        "                                               shuffle=False,\n",
        "                                               pin_memory=True)\n",
        "    test_set = datasets.CIFAR10(root=\"./data\", train=False,\n",
        "                                download=True, transform=transform_test)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                              num_workers=2,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              pin_memory=True)\n",
        "    training_criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    model = VGG11()\n",
        "    model.to(device)\n",
        "    # Adding DDP model\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01,\n",
        "                          momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    for epoch in range(40):\n",
        "        train_model(model, train_loader, optimizer, training_criterion, epoch)\n",
        "        test_model(model, test_loader, training_criterion)\n",
        "\n",
        "    cleanup()\n",
        "    scheduler.step()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thj1N4_b7it1",
        "outputId": "4ff5aca1-1321-4019-def1-06e9bb94eda1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,    40] loss: 1.816\n",
            "[1,    80] loss: 1.514\n",
            "[1,   120] loss: 1.309\n",
            "[1,   160] loss: 1.185\n",
            "Test set: Average loss: 1.1205, Accuracy: 6046/10000 (60%)\n",
            "\n",
            "[2,    40] loss: 1.050\n",
            "[2,    80] loss: 0.983\n",
            "[2,   120] loss: 0.943\n",
            "[2,   160] loss: 0.881\n",
            "Test set: Average loss: 0.9883, Accuracy: 6598/10000 (66%)\n",
            "\n",
            "[3,    40] loss: 0.834\n",
            "[3,    80] loss: 0.806\n",
            "[3,   120] loss: 0.756\n",
            "[3,   160] loss: 0.749\n",
            "Test set: Average loss: 0.8780, Accuracy: 7033/10000 (70%)\n",
            "\n",
            "[4,    40] loss: 0.717\n",
            "[4,    80] loss: 0.703\n",
            "[4,   120] loss: 0.662\n",
            "[4,   160] loss: 0.656\n",
            "Test set: Average loss: 0.7633, Accuracy: 7366/10000 (74%)\n",
            "\n",
            "[5,    40] loss: 0.624\n",
            "[5,    80] loss: 0.618\n",
            "[5,   120] loss: 0.585\n",
            "[5,   160] loss: 0.599\n",
            "Test set: Average loss: 0.7730, Accuracy: 7340/10000 (73%)\n",
            "\n",
            "[6,    40] loss: 0.562\n",
            "[6,    80] loss: 0.568\n",
            "[6,   120] loss: 0.528\n",
            "[6,   160] loss: 0.548\n",
            "Test set: Average loss: 0.6431, Accuracy: 7807/10000 (78%)\n",
            "\n",
            "[7,    40] loss: 0.524\n",
            "[7,    80] loss: 0.526\n",
            "[7,   120] loss: 0.485\n",
            "[7,   160] loss: 0.499\n",
            "Test set: Average loss: 0.6939, Accuracy: 7686/10000 (77%)\n",
            "\n",
            "[8,    40] loss: 0.485\n",
            "[8,    80] loss: 0.480\n",
            "[8,   120] loss: 0.448\n",
            "[8,   160] loss: 0.465\n",
            "Test set: Average loss: 0.6269, Accuracy: 7959/10000 (80%)\n",
            "\n",
            "[9,    40] loss: 0.454\n",
            "[9,    80] loss: 0.451\n",
            "[9,   120] loss: 0.425\n",
            "[9,   160] loss: 0.430\n",
            "Test set: Average loss: 0.6278, Accuracy: 8018/10000 (80%)\n",
            "\n",
            "[10,    40] loss: 0.416\n",
            "[10,    80] loss: 0.435\n",
            "[10,   120] loss: 0.388\n",
            "[10,   160] loss: 0.401\n",
            "Test set: Average loss: 0.5717, Accuracy: 8139/10000 (81%)\n",
            "\n",
            "[11,    40] loss: 0.406\n",
            "[11,    80] loss: 0.397\n",
            "[11,   120] loss: 0.375\n",
            "[11,   160] loss: 0.391\n",
            "Test set: Average loss: 0.5536, Accuracy: 8225/10000 (82%)\n",
            "\n",
            "[12,    40] loss: 0.378\n",
            "[12,    80] loss: 0.385\n",
            "[12,   120] loss: 0.346\n",
            "[12,   160] loss: 0.350\n",
            "Test set: Average loss: 0.5704, Accuracy: 8066/10000 (81%)\n",
            "\n",
            "[13,    40] loss: 0.361\n",
            "[13,    80] loss: 0.366\n",
            "[13,   120] loss: 0.327\n",
            "[13,   160] loss: 0.334\n",
            "Test set: Average loss: 0.6063, Accuracy: 8115/10000 (81%)\n",
            "\n",
            "[14,    40] loss: 0.331\n",
            "[14,    80] loss: 0.352\n",
            "[14,   120] loss: 0.316\n",
            "[14,   160] loss: 0.316\n",
            "Test set: Average loss: 0.6314, Accuracy: 8047/10000 (80%)\n",
            "\n",
            "[15,    40] loss: 0.313\n",
            "[15,    80] loss: 0.321\n",
            "[15,   120] loss: 0.294\n",
            "[15,   160] loss: 0.302\n",
            "Test set: Average loss: 0.6104, Accuracy: 8091/10000 (81%)\n",
            "\n",
            "[16,    40] loss: 0.296\n",
            "[16,    80] loss: 0.314\n",
            "[16,   120] loss: 0.286\n",
            "[16,   160] loss: 0.271\n",
            "Test set: Average loss: 0.6499, Accuracy: 8053/10000 (81%)\n",
            "\n",
            "[17,    40] loss: 0.288\n",
            "[17,    80] loss: 0.284\n",
            "[17,   120] loss: 0.265\n",
            "[17,   160] loss: 0.268\n",
            "Test set: Average loss: 0.6208, Accuracy: 8185/10000 (82%)\n",
            "\n",
            "[18,    40] loss: 0.263\n",
            "[18,    80] loss: 0.283\n",
            "[18,   120] loss: 0.262\n",
            "[18,   160] loss: 0.254\n",
            "Test set: Average loss: 0.6444, Accuracy: 8077/10000 (81%)\n",
            "\n",
            "[19,    40] loss: 0.248\n",
            "[19,    80] loss: 0.261\n",
            "[19,   120] loss: 0.244\n",
            "[19,   160] loss: 0.250\n",
            "Test set: Average loss: 0.5971, Accuracy: 8205/10000 (82%)\n",
            "\n",
            "[20,    40] loss: 0.251\n",
            "[20,    80] loss: 0.238\n",
            "[20,   120] loss: 0.235\n",
            "[20,   160] loss: 0.228\n",
            "Test set: Average loss: 0.5878, Accuracy: 8261/10000 (83%)\n",
            "\n",
            "[21,    40] loss: 0.224\n",
            "[21,    80] loss: 0.243\n",
            "[21,   120] loss: 0.228\n",
            "[21,   160] loss: 0.221\n",
            "Test set: Average loss: 0.6497, Accuracy: 8154/10000 (82%)\n",
            "\n",
            "[22,    40] loss: 0.231\n",
            "[22,    80] loss: 0.224\n",
            "[22,   120] loss: 0.202\n",
            "[22,   160] loss: 0.213\n",
            "Test set: Average loss: 0.6164, Accuracy: 8282/10000 (83%)\n",
            "\n",
            "[23,    40] loss: 0.214\n",
            "[23,    80] loss: 0.229\n",
            "[23,   120] loss: 0.194\n",
            "[23,   160] loss: 0.199\n",
            "Test set: Average loss: 0.5813, Accuracy: 8366/10000 (84%)\n",
            "\n",
            "[24,    40] loss: 0.215\n",
            "[24,    80] loss: 0.205\n",
            "[24,   120] loss: 0.188\n",
            "[24,   160] loss: 0.190\n",
            "Test set: Average loss: 0.5054, Accuracy: 8494/10000 (85%)\n",
            "\n",
            "[25,    40] loss: 0.193\n",
            "[25,    80] loss: 0.206\n",
            "[25,   120] loss: 0.195\n",
            "[25,   160] loss: 0.178\n",
            "Test set: Average loss: 0.5635, Accuracy: 8413/10000 (84%)\n",
            "\n",
            "[26,    40] loss: 0.187\n",
            "[26,    80] loss: 0.196\n",
            "[26,   120] loss: 0.184\n",
            "[26,   160] loss: 0.172\n",
            "Test set: Average loss: 0.4830, Accuracy: 8628/10000 (86%)\n",
            "\n",
            "[27,    40] loss: 0.170\n",
            "[27,    80] loss: 0.180\n",
            "[27,   120] loss: 0.175\n",
            "[27,   160] loss: 0.165\n",
            "Test set: Average loss: 0.6247, Accuracy: 8420/10000 (84%)\n",
            "\n",
            "[28,    40] loss: 0.179\n",
            "[28,    80] loss: 0.171\n",
            "[28,   120] loss: 0.157\n",
            "[28,   160] loss: 0.163\n",
            "Test set: Average loss: 0.5550, Accuracy: 8528/10000 (85%)\n",
            "\n",
            "[29,    40] loss: 0.164\n",
            "[29,    80] loss: 0.163\n",
            "[29,   120] loss: 0.162\n",
            "[29,   160] loss: 0.158\n",
            "Test set: Average loss: 0.5183, Accuracy: 8547/10000 (85%)\n",
            "\n",
            "[30,    40] loss: 0.157\n",
            "[30,    80] loss: 0.152\n",
            "[30,   120] loss: 0.144\n",
            "[30,   160] loss: 0.135\n",
            "Test set: Average loss: 0.5032, Accuracy: 8621/10000 (86%)\n",
            "\n",
            "[31,    40] loss: 0.152\n",
            "[31,    80] loss: 0.152\n",
            "[31,   120] loss: 0.134\n",
            "[31,   160] loss: 0.131\n",
            "Test set: Average loss: 0.5673, Accuracy: 8425/10000 (84%)\n",
            "\n",
            "[32,    40] loss: 0.142\n",
            "[32,    80] loss: 0.153\n",
            "[32,   120] loss: 0.128\n",
            "[32,   160] loss: 0.130\n",
            "Test set: Average loss: 0.5611, Accuracy: 8577/10000 (86%)\n",
            "\n",
            "[33,    40] loss: 0.135\n",
            "[33,    80] loss: 0.128\n",
            "[33,   120] loss: 0.129\n",
            "[33,   160] loss: 0.124\n",
            "Test set: Average loss: 0.5234, Accuracy: 8635/10000 (86%)\n",
            "\n",
            "[34,    40] loss: 0.121\n",
            "[34,    80] loss: 0.119\n",
            "[34,   120] loss: 0.116\n",
            "[34,   160] loss: 0.114\n",
            "Test set: Average loss: 0.5438, Accuracy: 8669/10000 (87%)\n",
            "\n",
            "[35,    40] loss: 0.118\n",
            "[35,    80] loss: 0.126\n",
            "[35,   120] loss: 0.117\n",
            "[35,   160] loss: 0.110\n",
            "Test set: Average loss: 0.6235, Accuracy: 8484/10000 (85%)\n",
            "\n",
            "[36,    40] loss: 0.107\n",
            "[36,    80] loss: 0.128\n",
            "[36,   120] loss: 0.108\n",
            "[36,   160] loss: 0.114\n",
            "Test set: Average loss: 0.6313, Accuracy: 8475/10000 (85%)\n",
            "\n",
            "[37,    40] loss: 0.100\n",
            "[37,    80] loss: 0.118\n",
            "[37,   120] loss: 0.107\n",
            "[37,   160] loss: 0.104\n",
            "Test set: Average loss: 0.6076, Accuracy: 8582/10000 (86%)\n",
            "\n",
            "[38,    40] loss: 0.101\n",
            "[38,    80] loss: 0.114\n",
            "[38,   120] loss: 0.099\n",
            "[38,   160] loss: 0.098\n",
            "Test set: Average loss: 0.5327, Accuracy: 8682/10000 (87%)\n",
            "\n",
            "[39,    40] loss: 0.087\n",
            "[39,    80] loss: 0.093\n",
            "[39,   120] loss: 0.089\n",
            "[39,   160] loss: 0.091\n",
            "Test set: Average loss: 0.5331, Accuracy: 8710/10000 (87%)\n",
            "\n",
            "[40,    40] loss: 0.098\n",
            "[40,    80] loss: 0.096\n",
            "[40,   120] loss: 0.090\n",
            "[40,   160] loss: 0.089\n",
            "Test set: Average loss: 0.5900, Accuracy: 8599/10000 (86%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}